{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "39ea4bed",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nltk'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 7\u001b[0m\n\u001b[1;32m      3\u001b[0m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA_VISIBLE_DEVICES\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mIPython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdisplay\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Audio\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m  \u001b[38;5;66;03m# we'll use this to split into sentences\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgeneration\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     11\u001b[0m     generate_text_semantic,\n\u001b[1;32m     12\u001b[0m     preload_models,\n\u001b[1;32m     13\u001b[0m )\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'nltk'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "\n",
    "from IPython.display import Audio\n",
    "import nltk  # we'll use this to split into sentences\n",
    "import numpy as np\n",
    "\n",
    "from bark.generation import (\n",
    "    generate_text_semantic,\n",
    "    preload_models,\n",
    ")\n",
    "from bark.api import semantic_to_waveform\n",
    "from bark import generate_audio, SAMPLE_RATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e9696234",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6eff84eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:10000 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoProcessor, BarkModel\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"suno/bark\")\n",
    "model = BarkModel.from_pretrained(\"suno/bark\")\n",
    "\n",
    "voice_preset = \"v2/en_speaker_6\"\n",
    "\n",
    "inputs = processor(\"Hello, my dog is cute\", voice_preset=voice_preset)\n",
    "\n",
    "audio_array = model.generate(**inputs)\n",
    "audio_array = audio_array.cpu().numpy().squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "776964b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "preload_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d03f4d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/huggingface/transformers.git\n",
      "  Cloning https://github.com/huggingface/transformers.git to /tmp/pip-req-build-qpzttcz0\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers.git /tmp/pip-req-build-qpzttcz0\n",
      "  Resolved https://github.com/huggingface/transformers.git to commit 481a95781404e48b1c80940be17e8279dec82fe8\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: filelock in /home/oscar/Repositories/home/.conda/lib/python3.11/site-packages (from transformers==4.42.0.dev0) (3.14.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /home/oscar/Repositories/home/.conda/lib/python3.11/site-packages (from transformers==4.42.0.dev0) (0.23.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/oscar/Repositories/home/.conda/lib/python3.11/site-packages (from transformers==4.42.0.dev0) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/oscar/Repositories/home/.conda/lib/python3.11/site-packages (from transformers==4.42.0.dev0) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/oscar/Repositories/home/.conda/lib/python3.11/site-packages (from transformers==4.42.0.dev0) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/oscar/Repositories/home/.conda/lib/python3.11/site-packages (from transformers==4.42.0.dev0) (2024.5.15)\n",
      "Requirement already satisfied: requests in /home/oscar/Repositories/home/.conda/lib/python3.11/site-packages (from transformers==4.42.0.dev0) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /home/oscar/Repositories/home/.conda/lib/python3.11/site-packages (from transformers==4.42.0.dev0) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/oscar/Repositories/home/.conda/lib/python3.11/site-packages (from transformers==4.42.0.dev0) (0.4.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/oscar/Repositories/home/.conda/lib/python3.11/site-packages (from transformers==4.42.0.dev0) (4.66.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/oscar/Repositories/home/.conda/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.23.0->transformers==4.42.0.dev0) (2024.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/oscar/Repositories/home/.conda/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.23.0->transformers==4.42.0.dev0) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/oscar/Repositories/home/.conda/lib/python3.11/site-packages (from requests->transformers==4.42.0.dev0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/oscar/Repositories/home/.conda/lib/python3.11/site-packages (from requests->transformers==4.42.0.dev0) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/oscar/Repositories/home/.conda/lib/python3.11/site-packages (from requests->transformers==4.42.0.dev0) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/oscar/Repositories/home/.conda/lib/python3.11/site-packages (from requests->transformers==4.42.0.dev0) (2024.2.2)\n",
      "Collecting git+https://github.com/suno-ai/bark.git\n",
      "  Cloning https://github.com/suno-ai/bark.git to /tmp/pip-req-build-onehl7ub\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/suno-ai/bark.git /tmp/pip-req-build-onehl7ub\n",
      "  Resolved https://github.com/suno-ai/bark.git to commit f4f32d4cd480dfec1c245d258174bc9bde3c2148\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting boto3 (from suno-bark==0.0.1a0)\n",
      "  Using cached boto3-1.34.108-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting encodec (from suno-bark==0.0.1a0)\n",
      "  Using cached encodec-0.1.1.tar.gz (3.7 MB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting funcy (from suno-bark==0.0.1a0)\n",
      "  Using cached funcy-2.0-py2.py3-none-any.whl.metadata (5.9 kB)\n",
      "Requirement already satisfied: huggingface-hub>=0.14.1 in /home/oscar/Repositories/home/.conda/lib/python3.11/site-packages (from suno-bark==0.0.1a0) (0.23.0)\n",
      "Requirement already satisfied: numpy in /home/oscar/Repositories/home/.conda/lib/python3.11/site-packages (from suno-bark==0.0.1a0) (1.26.4)\n",
      "Collecting scipy (from suno-bark==0.0.1a0)\n",
      "  Downloading scipy-1.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m893.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tokenizers in /home/oscar/Repositories/home/.conda/lib/python3.11/site-packages (from suno-bark==0.0.1a0) (0.19.1)\n",
      "Collecting torch (from suno-bark==0.0.1a0)\n",
      "  Downloading torch-2.3.0-cp311-cp311-manylinux1_x86_64.whl.metadata (26 kB)\n",
      "Requirement already satisfied: tqdm in /home/oscar/Repositories/home/.conda/lib/python3.11/site-packages (from suno-bark==0.0.1a0) (4.66.4)\n",
      "Requirement already satisfied: transformers in /home/oscar/Repositories/home/.conda/lib/python3.11/site-packages (from suno-bark==0.0.1a0) (4.42.0.dev0)\n",
      "Requirement already satisfied: filelock in /home/oscar/Repositories/home/.conda/lib/python3.11/site-packages (from huggingface-hub>=0.14.1->suno-bark==0.0.1a0) (3.14.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/oscar/Repositories/home/.conda/lib/python3.11/site-packages (from huggingface-hub>=0.14.1->suno-bark==0.0.1a0) (2024.5.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/oscar/Repositories/home/.conda/lib/python3.11/site-packages (from huggingface-hub>=0.14.1->suno-bark==0.0.1a0) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/oscar/Repositories/home/.conda/lib/python3.11/site-packages (from huggingface-hub>=0.14.1->suno-bark==0.0.1a0) (6.0.1)\n",
      "Requirement already satisfied: requests in /home/oscar/Repositories/home/.conda/lib/python3.11/site-packages (from huggingface-hub>=0.14.1->suno-bark==0.0.1a0) (2.31.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/oscar/Repositories/home/.conda/lib/python3.11/site-packages (from huggingface-hub>=0.14.1->suno-bark==0.0.1a0) (4.11.0)\n",
      "Collecting botocore<1.35.0,>=1.34.108 (from boto3->suno-bark==0.0.1a0)\n",
      "  Using cached botocore-1.34.108-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting jmespath<2.0.0,>=0.7.1 (from boto3->suno-bark==0.0.1a0)\n",
      "  Using cached jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting s3transfer<0.11.0,>=0.10.0 (from boto3->suno-bark==0.0.1a0)\n",
      "  Using cached s3transfer-0.10.1-py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting torchaudio (from encodec->suno-bark==0.0.1a0)\n",
      "  Downloading torchaudio-2.3.0-cp311-cp311-manylinux1_x86_64.whl.metadata (6.4 kB)\n",
      "Collecting einops (from encodec->suno-bark==0.0.1a0)\n",
      "  Using cached einops-0.8.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting sympy (from torch->suno-bark==0.0.1a0)\n",
      "  Using cached sympy-1.12-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch->suno-bark==0.0.1a0)\n",
      "  Using cached networkx-3.3-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting jinja2 (from torch->suno-bark==0.0.1a0)\n",
      "  Using cached jinja2-3.1.4-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch->suno-bark==0.0.1a0)\n",
      "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch->suno-bark==0.0.1a0)\n",
      "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch->suno-bark==0.0.1a0)\n",
      "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch->suno-bark==0.0.1a0)\n",
      "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch->suno-bark==0.0.1a0)\n",
      "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch->suno-bark==0.0.1a0)\n",
      "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.2.106 (from torch->suno-bark==0.0.1a0)\n",
      "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch->suno-bark==0.0.1a0)\n",
      "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch->suno-bark==0.0.1a0)\n",
      "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nccl-cu12==2.20.5 (from torch->suno-bark==0.0.1a0)\n",
      "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.1.105 (from torch->suno-bark==0.0.1a0)\n",
      "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting triton==2.3.0 (from torch->suno-bark==0.0.1a0)\n",
      "  Downloading triton-2.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
      "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch->suno-bark==0.0.1a0)\n",
      "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/oscar/Repositories/home/.conda/lib/python3.11/site-packages (from transformers->suno-bark==0.0.1a0) (2024.5.15)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/oscar/Repositories/home/.conda/lib/python3.11/site-packages (from transformers->suno-bark==0.0.1a0) (0.4.3)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/oscar/Repositories/home/.conda/lib/python3.11/site-packages (from botocore<1.35.0,>=1.34.108->boto3->suno-bark==0.0.1a0) (2.9.0)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /home/oscar/Repositories/home/.conda/lib/python3.11/site-packages (from botocore<1.35.0,>=1.34.108->boto3->suno-bark==0.0.1a0) (2.2.1)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch->suno-bark==0.0.1a0)\n",
      "  Downloading MarkupSafe-2.1.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/oscar/Repositories/home/.conda/lib/python3.11/site-packages (from requests->huggingface-hub>=0.14.1->suno-bark==0.0.1a0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/oscar/Repositories/home/.conda/lib/python3.11/site-packages (from requests->huggingface-hub>=0.14.1->suno-bark==0.0.1a0) (3.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/oscar/Repositories/home/.conda/lib/python3.11/site-packages (from requests->huggingface-hub>=0.14.1->suno-bark==0.0.1a0) (2024.2.2)\n",
      "Collecting mpmath>=0.19 (from sympy->torch->suno-bark==0.0.1a0)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: six>=1.5 in /home/oscar/Repositories/home/.conda/lib/python3.11/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.35.0,>=1.34.108->boto3->suno-bark==0.0.1a0) (1.16.0)\n",
      "Using cached boto3-1.34.108-py3-none-any.whl (139 kB)\n",
      "Using cached funcy-2.0-py2.py3-none-any.whl (30 kB)\n",
      "Downloading scipy-1.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading torch-2.3.0-cp311-cp311-manylinux1_x86_64.whl (779.2 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m779.2/779.2 MB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0m eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
      "Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
      "Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
      "Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
      "Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "Downloading triton-2.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (168.1 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.1/168.1 MB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0m eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached botocore-1.34.108-py3-none-any.whl (12.2 MB)\n",
      "Using cached jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Using cached s3transfer-0.10.1-py3-none-any.whl (82 kB)\n",
      "Using cached einops-0.8.0-py3-none-any.whl (43 kB)\n",
      "Using cached jinja2-3.1.4-py3-none-any.whl (133 kB)\n",
      "Using cached networkx-3.3-py3-none-any.whl (1.7 MB)\n",
      "Using cached sympy-1.12-py3-none-any.whl (5.7 MB)\n",
      "Downloading torchaudio-2.3.0-cp311-cp311-manylinux1_x86_64.whl (3.4 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m30.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading MarkupSafe-2.1.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (28 kB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "Building wheels for collected packages: suno-bark, encodec\n",
      "  Building wheel for suno-bark (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for suno-bark: filename=suno_bark-0.0.1a0-py3-none-any.whl size=2567358 sha256=984bd354b3709b58654fc7f208278642ede437cd39f4d825f347ea117fd7b10d\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-_bdbj2ul/wheels/d7/12/5e/2c493bffde263f84a51c141ad68c113365714a1e0ef0051c9b\n",
      "  Building wheel for encodec (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for encodec: filename=encodec-0.1.1-py3-none-any.whl size=45762 sha256=f50f54a7a0616b1a930256cd6e89836dcd6a843f5b1f47590064dde31aa307c0\n",
      "  Stored in directory: /home/oscar/.cache/pip/wheels/b4/a4/88/480018a664e58ca7ce6708759193ee51b017b3b72aa3df8a85\n",
      "Successfully built suno-bark encodec\n",
      "Installing collected packages: mpmath, funcy, triton, sympy, scipy, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, networkx, MarkupSafe, jmespath, einops, nvidia-cusparse-cu12, nvidia-cudnn-cu12, jinja2, botocore, s3transfer, nvidia-cusolver-cu12, torch, boto3, torchaudio, encodec, suno-bark\n",
      "Successfully installed MarkupSafe-2.1.5 boto3-1.34.108 botocore-1.34.108 einops-0.8.0 encodec-0.1.1 funcy-2.0 jinja2-3.1.4 jmespath-1.0.1 mpmath-1.3.0 networkx-3.3 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 s3transfer-0.10.1 scipy-1.13.0 suno-bark-0.0.1a0 sympy-1.12 torch-2.3.0 torchaudio-2.3.0 triton-2.3.0\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/huggingface/transformers.git\n",
    "!pip install git+https://github.com/suno-ai/bark.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a025a4",
   "metadata": {},
   "source": [
    "# Simple Long-Form Generation\n",
    "We split longer text into sentences using `nltk` and generate the sentences one by one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "57b06e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "script = \"\"\"\n",
    "Hey, have you heard about this new text-to-audio model called \"Bark\"? \n",
    "Apparently, it's the most realistic and natural-sounding text-to-audio model \n",
    "out there right now. People are saying it sounds just like a real person speaking. \n",
    "I think it uses advanced machine learning algorithms to analyze and understand the \n",
    "nuances of human speech, and then replicates those nuances in its own speech output. \n",
    "It's pretty impressive, and I bet it could be used for things like audiobooks or podcasts. \n",
    "In fact, I heard that some publishers are already starting to use Bark to create audiobooks. \n",
    "It would be like having your own personal voiceover artist. I really think Bark is going to \n",
    "be a game-changer in the world of text-to-audio technology.\n",
    "\"\"\".replace(\"\\n\", \" \").strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f747f804",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = nltk.sent_tokenize(script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "17400a9b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████| 100/100 [00:02<00:00, 43.03it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████| 17/17 [00:06<00:00,  2.45it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████| 100/100 [00:04<00:00, 22.73it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████| 33/33 [00:13<00:00,  2.52it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████| 100/100 [00:01<00:00, 66.30it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████| 11/11 [00:04<00:00,  2.46it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████| 100/100 [00:04<00:00, 20.99it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████| 35/35 [00:14<00:00,  2.46it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████| 100/100 [00:03<00:00, 25.63it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████| 29/29 [00:11<00:00,  2.50it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████| 100/100 [00:04<00:00, 23.90it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████| 30/30 [00:12<00:00,  2.46it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████| 100/100 [00:01<00:00, 53.24it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████| 14/14 [00:05<00:00,  2.51it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████| 100/100 [00:01<00:00, 50.63it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████| 15/15 [00:05<00:00,  2.57it/s]\n"
     ]
    }
   ],
   "source": [
    "SPEAKER = \"v2/en_speaker_6\"\n",
    "silence = np.zeros(int(0.25 * SAMPLE_RATE))  # quarter second of silence\n",
    "\n",
    "pieces = []\n",
    "for sentence in sentences:\n",
    "    audio_array = generate_audio(sentence, history_prompt=SPEAKER)\n",
    "    pieces += [audio_array, silence.copy()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04cf77f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(np.concatenate(pieces), rate=SAMPLE_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2d4625",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6d13249b",
   "metadata": {},
   "source": [
    "# $ \\\\ $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdfc8bf5",
   "metadata": {},
   "source": [
    "# Advanced Long-Form Generation\n",
    "Somtimes Bark will hallucinate a little extra audio at the end of the prompt.\n",
    "We can solve this issue by lowering the threshold for bark to stop generating text. \n",
    "We use the `min_eos_p` kwarg in `generate_text_semantic`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "62807fd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████| 100/100 [00:02<00:00, 38.05it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████| 18/18 [00:07<00:00,  2.46it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████| 100/100 [00:03<00:00, 32.28it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████| 21/21 [00:08<00:00,  2.54it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████| 100/100 [00:01<00:00, 55.78it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████| 14/14 [00:05<00:00,  2.57it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████| 100/100 [00:06<00:00, 14.73it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████| 35/35 [00:14<00:00,  2.47it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████| 100/100 [00:02<00:00, 40.29it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████| 18/18 [00:07<00:00,  2.56it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████| 100/100 [00:03<00:00, 32.92it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████| 20/20 [00:08<00:00,  2.47it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████| 100/100 [00:01<00:00, 68.87it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████| 12/12 [00:04<00:00,  2.62it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████| 100/100 [00:02<00:00, 47.64it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████| 15/15 [00:06<00:00,  2.46it/s]\n"
     ]
    }
   ],
   "source": [
    "GEN_TEMP = 0.6\n",
    "SPEAKER = \"v2/en_speaker_6\"\n",
    "silence = np.zeros(int(0.25 * SAMPLE_RATE))  # quarter second of silence\n",
    "\n",
    "pieces = []\n",
    "for sentence in sentences:\n",
    "    semantic_tokens = generate_text_semantic(\n",
    "        sentence,\n",
    "        history_prompt=SPEAKER,\n",
    "        temp=GEN_TEMP,\n",
    "        min_eos_p=0.05,  # this controls how likely the generation is to end\n",
    "    )\n",
    "\n",
    "    audio_array = semantic_to_waveform(semantic_tokens, history_prompt=SPEAKER,)\n",
    "    pieces += [audio_array, silence.copy()]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133fec46",
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(np.concatenate(pieces), rate=SAMPLE_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eee9f5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "be8e125e",
   "metadata": {},
   "source": [
    "# $ \\\\ $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a16c1b",
   "metadata": {},
   "source": [
    "# Make a Long-Form Dialog with Bark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c5eff8",
   "metadata": {},
   "source": [
    "### Step 1: Format a script and speaker lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5238b297",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Samantha: Hey, have you heard about this new text-to-audio model called \"Bark\"?',\n",
       " \"John: No, I haven't. What's so special about it?\",\n",
       " \"Samantha: Well, apparently it's the most realistic and natural-sounding text-to-audio model out there right now. People are saying it sounds just like a real person speaking.\",\n",
       " 'John: Wow, that sounds amazing. How does it work?',\n",
       " 'Samantha: I think it uses advanced machine learning algorithms to analyze and understand the nuances of human speech, and then replicates those nuances in its own speech output.',\n",
       " \"John: That's pretty impressive. Do you think it could be used for things like audiobooks or podcasts?\",\n",
       " 'Samantha: Definitely! In fact, I heard that some publishers are already starting to use Bark to create audiobooks. And I bet it would be great for podcasts too.',\n",
       " 'John: I can imagine. It would be like having your own personal voiceover artist.',\n",
       " 'Samantha: Exactly! I think Bark is going to be a game-changer in the world of text-to-audio technology.']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speaker_lookup = {\"Samantha\": \"v2/en_speaker_9\", \"John\": \"v2/en_speaker_2\"}\n",
    "\n",
    "# Script generated by chat GPT\n",
    "script = \"\"\"\n",
    "Samantha: Hey, have you heard about this new text-to-audio model called \"Bark\"?\n",
    "\n",
    "John: No, I haven't. What's so special about it?\n",
    "\n",
    "Samantha: Well, apparently it's the most realistic and natural-sounding text-to-audio model out there right now. People are saying it sounds just like a real person speaking.\n",
    "\n",
    "John: Wow, that sounds amazing. How does it work?\n",
    "\n",
    "Samantha: I think it uses advanced machine learning algorithms to analyze and understand the nuances of human speech, and then replicates those nuances in its own speech output.\n",
    "\n",
    "John: That's pretty impressive. Do you think it could be used for things like audiobooks or podcasts?\n",
    "\n",
    "Samantha: Definitely! In fact, I heard that some publishers are already starting to use Bark to create audiobooks. And I bet it would be great for podcasts too.\n",
    "\n",
    "John: I can imagine. It would be like having your own personal voiceover artist.\n",
    "\n",
    "Samantha: Exactly! I think Bark is going to be a game-changer in the world of text-to-audio technology.\"\"\"\n",
    "script = script.strip().split(\"\\n\")\n",
    "script = [s.strip() for s in script if s]\n",
    "script"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee547efd",
   "metadata": {},
   "source": [
    "### Step 2: Generate the audio for every speaker turn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "203e5081",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████| 100/100 [00:02<00:00, 34.03it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████| 22/22 [00:08<00:00,  2.55it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████| 100/100 [00:01<00:00, 71.58it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████| 11/11 [00:04<00:00,  2.65it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████| 100/100 [00:04<00:00, 22.75it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████| 33/33 [00:13<00:00,  2.53it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████| 100/100 [00:01<00:00, 70.76it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████| 11/11 [00:04<00:00,  2.63it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████| 100/100 [00:04<00:00, 20.46it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████| 36/36 [00:14<00:00,  2.47it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████| 100/100 [00:04<00:00, 20.18it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████| 37/37 [00:14<00:00,  2.51it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████| 100/100 [00:04<00:00, 23.04it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████| 32/32 [00:12<00:00,  2.48it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████| 100/100 [00:01<00:00, 54.64it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████| 14/14 [00:05<00:00,  2.58it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████| 100/100 [00:03<00:00, 31.71it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████| 24/24 [00:09<00:00,  2.56it/s]\n"
     ]
    }
   ],
   "source": [
    "pieces = []\n",
    "silence = np.zeros(int(0.5*SAMPLE_RATE))\n",
    "for line in script:\n",
    "    speaker, text = line.split(\": \")\n",
    "    audio_array = generate_audio(text, history_prompt=speaker_lookup[speaker], )\n",
    "    pieces += [audio_array, silence.copy()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c54bada",
   "metadata": {},
   "source": [
    "### Step 3: Concatenate all of the audio and play it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a56842",
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(np.concatenate(pieces), rate=SAMPLE_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1bc5877",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
